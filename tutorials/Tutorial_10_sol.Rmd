---
title: "Lab 10 - Mixed Models"
author: "Adam Smith"
output: 
  html_document:
    number_sections: yes
    toc: yes
    toc_float: yes
    df_print: paged
    code_download: true
---

```{css, echo=TRUE, class.source="bg-danger"}
#answer {
  background-color: #C5E4F3;
  border-left: 3px solid #297EB1;
  padding: 10px;
}
```

```{r setup_rmd, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# knitr::opts_knit$set(root.dir = "../data")
knitr::opts_chunk$set(fig.dim=c(5,3.5), out.width="70%", fig.retina=2)
```

Note: you can download the .Rmd file for this tutorial using the 'Code' button on the top right of this page.

There are a few libraries you might need to install (with e.g. `install.packages("asbio")`).

```{r message=FALSE}
library(tidyverse)
library(ggplot2)
library(lme4)
library(asbio)
library(glmmTMB)
library(marginaleffects)
theme_set(theme_bw())
```


# Tutorial: Mixed models of potato yields

This example is from page 464 of Aho's book [Foundational and Applied Statistics for Biologists using R](https://www.routledge.com/Foundational-and-Applied-Statistics-for-Biologists-Using-R/Aho/p/book/9781439873380), which is available as an e-book or physical copy from the Massey library. It is a useful resource and covers quite a lot of the content of this course. It has an associated R package, `asbio`, which you can install with `install.packages("asbio")`. 

## Load data and analyse the experimental design 

```{r, echo=TRUE}
data(potato)
# If you can't install the 'asbio' package, you can download "potato.txt" from STREAM
# and load it with `potato <- read.table("potato.txt")`

# reorder the factor Variety according to the mean of Yield 
# (this makes our plots look nicer)
potato <- potato %>% mutate(Variety = fct_reorder(Variety, Yield, mean))

str(potato)
```
The `Yield` of potato is the response variable. There are 108 observations, one continuous response variable, and three factors: 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`Variety`, the variety of the potato plant,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`Fert` is the fertiliser that was used, and
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`Patch` identifies the piece of ground that the observation came from.


We will model `Yield` based on `Variety` and `Fert`. 

There are over 4000 potato varieties, and so we might wish to make inference about variation  the population of varieties. If we were interested specifically in the 12 varieties in the experiment, then we'd treat `Variety` as a fixed effect. Treatment factors, like fertiliser, are almost always treated as fixed. 

The three fertilisers are `B` = 'basal manure', a control treatment; `Cl` = 'chloride addition'; and `S` = 'sulfur addition'.

For now, we're going to ignore the factor `Patch`. Ordinarily, we wouldn't do this because there may be some variation associated with this factor that should be accounted for in the model. 

The interaction between `Fert` and `Variety` is a random effect, which captures the differences in the effects of fertilisers among the varieties. Any interaction involving a random effect is itself a random effect. 

First, let's see if these two factors are crossed or nested.

```{r}
table(potato$Variety, potato$Fert)
```

Although it's not a balanced design, `Fert` and `Variety` are crossed because there are data points in each combination of the levels of the two factors - every variety was grown with every fertiliser.

Here's a basic plot of the data for each fertiliser, with a panel for each variety. 

```{r fig.dim=c(8,5), out.width="90%"}
potato %>%
  ggplot() +
  aes(x=Fert, y=Yield, colour=Fert) +
  geom_point() +
  facet_wrap(~Variety) +
  xlab("Fertiliser")
```

From the plot, it doesn't look like the additions to the fertiliser are particularly effective. 

Now we'll make a plot that'll allows us to see differences among varieties.

```{r fig.dim=c(8,5), out.width="90%"}
potato %>%
  ggplot() +
  aes(x=Variety, y=Yield, colour= Fert) +
  geom_point() +
  # facet_wrap(~Fert) +
  xlab("Variety") +
  coord_flip()
  
```

It looks like Variety has a greater effect on Yield.

## Model description

A model including both fixed and random factors is called a *mixed model*.

We will model `Yield` with three terms: 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\text{A}$: the random effects of `Variety`,  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\text{B}$: the fixed effects of `Fert`, and

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\text{A:B}$: the random `Variety:Fert` interaction. 

The full model may be written as follows.

$$
y_{ijk} = \mu + \alpha_i + \beta_j + \gamma_{ij} + \varepsilon_{ijk} \\
\beta_j \sim N(0,\sigma^2_\text{B}) \\
\gamma_{ij} \sim N(0,\sigma^2_\text{A:B}) \\
\varepsilon_{ijk} \sim N(0,\sigma^2_{\text{E}}) 
$$

where 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$y_{ijk}$ is the yield of replicate $k$ from variety $j$ with fertiliser $i$, 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\mu$ is the global mean,

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\alpha_i$ is the effect of fertiliser $i$ (factor $\text{A}$),

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\beta_j$ is the effect of variety $j$ (factor $\text{B}$),

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\gamma_{ij}$ is the interaction effect of being in a particular combination of fertiliser $i$ and variety $j$ ($\text{A:B}$ interaction),   

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\varepsilon_{ijk}$ is the random individual error for observation $k$ in the group with fertiliser $i$ and variety $j$,  

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\sigma_{\text{B}}^2$ is the variance component for variety (i.e., $\text{Var}(\beta_j)$),

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\sigma_{\text{A:B}}^2$ is the variance component for the variety-by-fertiliser interaction (i.e., $\text{Var}(\gamma_{ij})$), 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\sigma_{\text{E}}^2$ is the variance of the residuals (i.e., $\text{Var}(\varepsilon_{ijk})$). 


These effects are additive, so that the mean for fertiliser $i$ and variety $j$ is obtained by adding the terms $\mu + \alpha_i + \beta_j + \gamma_{ij}$.


## Classical ANOVA

Let us calculate the variance components using the ANOVA method. When we have one fixed and one random factor, the ANOVA estimators for a balanced two-way factorial design are given in the following table.

For A fixed and B random in a balanced design:

| Source | Expected mean squares | ANOVA estimators |
|:-------|:---------------------------------------------------------------------------------------------------------------------------------------------:|---------:|
| A      | $\text{E(MS}_\text{A})=\sigma_{\text{E}}^2 + n\sigma_\text{A:B}^2 + bn\frac{\sum \alpha^2}{a-1}$                                                         |          |
| B      | $\text{E(MS}_\text{B}) = \sigma_{\text{E}}^2 + an\sigma_\text{B}^2$                                  | $\hat\sigma_\text{B}^2= \frac{\text{E(MS}_\text{B}) - \text{E(MS}_\text{E})}{an}$ |
| A:B    | $\text{E(MS}_\text{A:B})= \sigma_{\text{E}}^2 + n\sigma_\text{A:B}^2$                          | $\hat\sigma_\text{A:B}^2=\frac{\text{E(MS}_\text{A:B}) - \text{E(MS}_\text{E})}{n}$         |
| Error  | $\text{E(MS}_\text{E})=\sigma_{\text{E}}^2$                                                         | $\hat\sigma_{\text{E}}^2= \text{E(MS}_\text{E})$                       |

where $a$ is the number of levels of factor $\text{A}$, and $b$ is the number of levels of factor $\text{B}$. 

Let's fit a classical two-way ANOVA model.

```{r, echo=TRUE}
m1 <- aov(Yield ~ Variety * Fert, data=potato)
anova(m1)
```

These tests implicitly treat every term as fixed, so we can ignore the test results. 

However, we can use this output to compute the variance components as follows.

```{r, echo=TRUE}
MS_fertiliser <- 0.1748
MS_variety <- 3.9671
MS_interaction <- 0.0996
MS_error <- 0.3544
n=3 #3 replicates for each combination of levels
a=3 # number of levels in the fixed factor A (Fert)
```

```{r, echo=TRUE}
(MS_variety - MS_error)/(a*n) # Variance component for Variety

(MS_interaction - MS_error)/n # Variance component for Fert:Variety

MS_error # Variance component for error
```

We see that one of the estimates is negative. Variances cannot be negative. A negative variance would imply that the effects are less than zero, that the group means are closer together than exactly the same.

Also, this design is *not* balanced, so we cannot really use these formulae in a formal analysis (we have just used n=3 above, but n varies from cell to cell). It is possible to adjust the ANOVA estimates according to an unbalanced design, but it is rather fiddly so we will not go into it here. Let's instead fit the model using more modern approaches.


## Fitting the full model with `lme4`

Fit the model using `lme4::lmer` function, with the code `(1 | Variety)` and `(1 | Variety : Fert)` to specify that these terms are random effects.

```{r, echo=TRUE}
lmer1_potato <- lmer(Yield ~ Fert + (1 | Variety) + (1 | Variety : Fert), data = potato)
summary(lmer1_potato)
```

The estimates of $\sigma_{Variety}^2$ and $\sigma_{Variety:Fert}^2$ are 0.41 and 0, respectively. The table also shows the standard deviations, $\sigma_{Variety}$ and $\sigma_{Variety:Fert}$, which are the square-roots of the variances. The estimated residual variance $\sigma_{\varepsilon}^2$ is 0.28.

The warning `boundary (singular) fit: see help('isSingular')` shouldn't be ignored. It suggests that the optimiser had some trouble as it was hitting some boundary in the parameter space. This will be related to the fact that the variance component for `Variety:Fert` was estimated to be zero. 

## Fitting the full model with `glmmTMB`

A more recent and very useful package for fitting mixed models is `glmmTMB` (because I know you wanted another mixed models package!). `glmmTMB` uses a slightly different optimiser, which is generally a bit more stable than `lmer`. It uses a Laplace approximation for random effects. 

```{r, echo=TRUE}
glmmTMB1_potato <- glmmTMB(Yield ~ Fert + (1 | Variety) + (1 | Variety : Fert), data = potato)
summary(glmmTMB1_potato)
```

Now, the estimated variance component for `Variety:Fert` isn't exactly zero, but it is very small. Should we remove it or keep it in the model? 

## Inference: testing hypotheses and model selection

It is important to distinguish between these two procedures (testing hypotheses and model selection) because the goals and methods are different.

Testing hypotheses is about weighing the evidence for the **existence** of an effect. More correctly, it is about weighing evidence **against** a null hypothesis that (usually) some parameter is zero. With hypothesis testing, we put the burden of evidence on the data to contradict the null hypothesis. If the test is inconclusive, then we're left with the null (though we do not assert the null is true). Basically, the null stands until there are data to show otherwise. The key tools for testing hypotheses are test statistics and p-values (or, sometimes, showing a confidence interval that doesn't contain zero). 

Model selection is more often about finding a good predictive model. There is no burden of evidence one way or another. We simply want to select a model that will most likely make the most accurate predictions for new data. We sometimes have terms in the model that are not significant, but look as though their presence in the model improves predictive accuracy. The key tools for model selection are information criteria, such as the AIC. Sometimes, we use more computationally intensive methods, such as leave-one-out or k-fold cross validation, which set aside parts of the data for testing the predictive accuracy of the model. 

**So, what's our purpose here?**

In our case, we wish to know if the different fertilisers have different average yields, and whether the effects of fertiliser differ among varieties (i.e., the interaction between fertiliser and yield). 

This sounds more like a hypothesis-testing situation. But do we need to do model selection in this case? With simple ANOVA-type models, we sometimes just leave all the terms in the model, even ones that aren't statistically significant. Excluding a term is asserting that it is exactly zero, which is an assumption we don't have to make. That said, when it is clear that a term is negligible, it is prudent to remove it to avoid the model from overfitting, thus improving the precision of the remaining terms.

As always, will we begin by examining the interaction term.

### Testing the random interaction between fertiliser and variety

Let's now test the hypothesis that the variance component for the interaction term is zero. That is, the fertiliser effects do not vary among varieties; their effects are the same for all varieties. In testing this hypothesis, we're looking for evidence in the data that the fertilisers affect some varieties differently to other varieties, thus contradicting the null.

$\text{H}_0: \sigma_{Variety:Fert}^2 = 0$

First, we'll use the `update` function to remove the term. In the code below, the dots `.` are just place holders for the formula of the original model `glmmTMB1_potato`. So `. ~ .` just means "the response and predictors as before", and then `- (1|Variety:Fert)` means "remove the random interaction between variety and fertiliser". 

```{r}
glmmTMB2_potato <- update(glmmTMB1_potato, . ~ . - (1|Variety:Fert))
summary(glmmTMB2_potato)
```

Now we'll use the `anova` function to apply a Likelihood Ratio Test. For two models, one nested in the other, the LRT tests whether the more complex model has significantly better fit to the data in terms of the difference in log-likelihood, taking into account the number of extra degrees of freedom it uses.

```{r}
anova(glmmTMB2_potato, glmmTMB1_potato)
```
In this case, the more complicated model (with the interaction) has the same log-likelihood as the simpler model, despite having an extra term in it. An extra term gives a model more flexibility to fit to the data. The fact that it doesn't improve the fit at all here means it is a complete dud. The p-value is 1. Clearly, we fail to reject the null hypothesis. 

Moreover, the lower AIC and BIC scores of the simpler model, `glmmTMB2`, mean that it is likely to make better predictions for new data.

So, we'll continue with the simpler model, without the interaction term. In doing so, we are assuming that $\sigma_{Variety:Fert}^2 = 0$. 


### Testing the fixed effect of fertiliser

We'll use the Likelihood Ratio Test again to test for an effect of fertiliser. The null hypothesis is that there is no differences in average yields among any of the fertiliser treatments.

$\text{H}_0:  \alpha_1 = \alpha_2 = \alpha_3 = 0$

We'll now fit model with the `Fert` term removed. 

```{r}
glmmTMB3_potato <- update(glmmTMB2_potato, . ~ . - Fert)
summary(glmmTMB3_potato)
```

And do the likelihood ratio test of the two models.

```{r}
anova(glmmTMB3_potato, glmmTMB2_potato)
```

The hypothesis test is not significant at the 5% level (P > 0.05). The model with the fertiliser term included doesn't fit the data much better than we would expect it to, just by chance, under the null hypothesis that fertiliser has no effect on yield. 

Moreover, the information criteria (AIC and BIC) indicate that there is little evidence that having fertiliser in the model helps to predict yield, as the model without fertiliser has lower AIC and BIC scores.

Now, should we retain `Fert` in our model when we test for `Variety`? The p-value was 0.55, which really is no evidence at all for a difference. However, this does not mean we can assume that that the fertiliser treatment does indeed have *no* effect. Absence of evidence is not evidence of absence, after all. 

In this situation, a commonly used rule of thumb for deciding whether to remove a non-significant term in a model is a p-value > 0.25. So, normally, we'd feel comfortable removing `Fert` from the model before testing `Variety`. However, in this case, we'll keep `Fert` in the model for the rest of the tutorial, just so we can use it to make plots of means, etc.


### Testing the random effect of variety

Next, we test for the random effect of variety. The null hypotheses is 
$H_0: \sigma_{Variety}^2= 0$.

The estimated variance component for `Variety` is 0.37, according to the model `glmmTMB2_potato` above.

Let's make the final model and do the test.


```{r}
glmmTMB4_potato <- update(glmmTMB2_potato, . ~ . - (1|Variety))
summary(glmmTMB4_potato)
```

```{r}
anova(glmmTMB4_potato, glmmTMB2_potato)
```

Now, that's convincing! With such a tiny p-value, we can confidently conclude that the average yields are indeed different among varieties.

*Wait, what?*

It is worthwhile stopping for a moment and asking: why did we test this hypothesis? 

The null hypothesis here is that all varieties of potato have exactly the same yield, on average. In other words, no variety has an average yield any different to any other variety. Does that sound realistic? I think not. It's a bit of a 'straw-man' hypothesis, really. There is actually no particular reason to have done this test. 

This is why, when I laid out our purpose above, I said nothing about testing for an effect of yield, only for the effects of fertiliser, and differences in the effects of fertiliser among varieties. Variety was included in the study to test the generality of the fertiliser effects, so it should be in the model, regardless of whether it is significant.  


## Model interpretation 

### Summary output 

Let's look at the summary of our working model again. 

```{r}
summary(glmmTMB2_potato)
```

The lower `Conditional model` table shows the fixed effects of `Fert`. There are three lines, one for each coefficient: `(Intercept)`, `FertCl`, and `FertS`. 

R has done its usual method of setting one level as the 'reference' level. This is a big difference between how random and fixed effects are parameterised. There is no reference level for a random effect, like there is for a fixed effect. Random effects sum to zero.

In the case of `Fert`, the reference level is `B` for 'basal manure'. So, the row `(Intercept)` gives the estimate of the mean yield of level `B`. In this case, the average yield for data points with basal manure was 2.95.

The next two rows are the estimated effects of the other two levels, relative to the reference level. 

The coefficient `FertCl` is the difference between the mean of level `Cl` (chloride addition) and the mean of reference level `B`. The estimated mean of level `Cl` is therefore 2.95 + 0.08 = 3.03. This effect has a large p-value, so there is no evidence that adding chloride affects yield, on average.

The coefficient `FertS` is the difference between the mean of level `S` (sulphur addition) and the mean of reference level `B`. The estimated mean of level `S` is therefore 2.95 + 0.14 = 3.09. The p-value > 0.05 indicates little evidence that adding sulphur affects yield.

These effects are estimated irrespective of the two random terms. They are not estimates for any particular variety. They are the estimated 'global' or 'marginal' means for each fertiliser, removing any effect of variety. 

However, that isn't to say that the tests of the effects of fertiliser do take the random terms into account. If we fit a model without the random effect of variety, we wouldn't get the same result. 

Let's see what happens to the tests when we omit `Variety` from the model altogether. 


```{r}
glmmTMB(Yield ~ Fert, data = potato)  |>  summary()
```

The estimates of the effects of fertiliser are the same if we remove the random effect of variety, but the standard errors have changed and the p-values have gone up!

A random effect, even if it's of no interest, can be very important. Remember, the precision of our estimates and the power of our hypothesis tests are hampered by unexplained variation. In the model with variety in it, the residual variance is 0.289; without variety, it is 0.651. That's a lot more unexplained variation. Therefore, if the model is fit ignoring variety, there's a lot more noise and the signal is less clear.

Also, excluding variety from the model means to completely ignore the fact that the experiment had more than one variety of potato in it. Sometimes, we retain factors in a model because they're an important part of the structure of the experiment, regardless on whether there's a lot of evidence that this structure is important. It is generally safer to avoid asserting an effect is zero. 

### Coefficients as means

We can easily change the way the model is parameterised. To fit a model where the coefficients are the means of each of the three fertilisers (rather than setting one coefficient as the intercept and the other two as differences to the intercept), we can use the following code:

```{r}
glmmTMB2_0_potato <- glmmTMB(Yield ~ 0 + Fert + (1 | Variety), data = potato) 

summary(glmmTMB2_0_potato)
```

The `0 +` in the forumula removes the intercept. Instead, the coefficients are simply the means for each `Fert` group. This parameterisation makes it easier for estimating and plotting means, but it is not so useful for testing hypotheses about differences among the fertiliser levels. The p-values now just test whether each mean is different from zero -- not a very useful test!


### Estimating and plotting means

It is always useful to accompany a formal analysis with tables and plots of various quantities predicted by the model. For example, we might wish to show the average yield for each type of fertiliser, averaged across the varieties. As scientists, we should add some intervals around those average to indicate our uncertainty in these quantities. But how do we get those intervals?

The answer to this question is not as straightforward as you might think. There are a few different types of intervals we could show, due to having the random effect of variety in the model.

There are lots of packages out there for calculating different types of predictions, including `emmeans`, `marginalmeans`, and `ggeffects`. 

#### Plotting confidence intervals for a simple model

Let's start by getting estimates of mean yields for the three fertilisers, based on a simple model of `Yield ~ Fertiliser`, ignoring varieties. We can use the good old `predict()` function.

```{r}
m1 <- lm(Yield ~ Fert, data = potato)

newdat <- data.frame(Fert = c("B","Cl","S"))

m1_p_ci <- predict(m1,
                   newdata = newdat,
                   interval = "confidence")

tab_p_ci <- data.frame(newdat, m1_p_ci)

tab_p_ci
```

The column `fit` gives the estimated mean yield for each of the three fertilisers, and the columns `lwr` and `upr` are the lower and upper bounds of the 95% confidence interval for the means, respectively.

We can plot these estimates and intervals.

```{r}
p <- tab_p_ci |> 
  ggplot() + 
  aes(x = Fert, y = fit, ymin = lwr, ymax = upr) +
  geom_pointrange()

p
```

And let's add the data to the plot:

```{r}
p + 
  geom_jitter(data = potato, 
              aes(x = Fert, y = Yield), 
              inherit.aes = F,
              alpha = .3, 
              width = .2)
  
```

We can get the same sort of plot straight from the model itself using the `visreg` package.

```{r}
visreg::visreg(m1, gg=T)
```


But we're ignoring something here... *Variety*!

```{r}
p + 
  geom_jitter(data = potato, 
              aes(x = Fert, y = Yield, colour = Variety), 
              inherit.aes = F,
              alpha = .4, 
              width = .2)
  
```

#### Plotting confidence intervals for a mixed-effects model

When we include *Variety* in the model, we get slightly larger confidence intervals for our estimates of *Fertiliser* means. 

```{r}
m1_TMB <- glmmTMB(Yield ~ 0 + Fert, data = potato)
m2_TMB <- glmmTMB(Yield ~ 0 + Fert + (1 | Variety), data = potato)

library(parameters)
library(see)

m1_m2_compare <- compare_parameters(m1_TMB, m2_TMB) 

m1_m2_compare

plot(m1_m2_compare)

```

This shows that including *Variety* as a random effect in the model as a random effect results in wider confidence intervals around the estimates of the *Fertiliser* means. 

The [`ggeffects`](https://strengejacke.github.io/ggeffects/index.html) library has some useful functions for calculating "marginal effects", taking into account other variables in the model. This isn't such a big deal with a fully balanced factorial design like the potato study, where we have an equal number of data points within each combination of levels of two factors. But it can be very useful for unbalanced designs, or where there is a quantitative predictor in the model. 

##### Marginal means with `ggeffect()`

`ggeffect()` returns *marginal* estimates of means; that is, estimates of means while marginalising over the levels of other factors. 

For example:

```{r}
library(ggeffects)

ggeffect(m2_TMB)
```
You can also make simple plots from a ggeffects object.

```{r}
ggeffect(m2_TMB) |> plot()
```


These points represent estimates of the average yield for each fertiliser, averaged over all the varieties, with confidence intervals.

##### Conditional means with `ggpredict()`

`ggpredict()` is another function generally used to predict *conditional* effects, based on all combinations of factors, or particular combinations of levels factors. 

Note that, if you want to include the random effects, you have to include the argument `type = "random"`. However, this automatically sets another argument `interval = "prediction"`, which means the intervals include the residual error as well. To get confidence intervals for conditional means (without residual error), including random effects, you must specify `type = "random"` ***and*** `interval = "confidence"`.

Here is the full list of confidence intervals for all combinations of `Fert` and `Variety`.

```{r}
# All combinations of Fert and Variety
ggpredict(m2_TMB, 
          terms = c("Fert", "Variety"), 
          type = "random",
          interval = "confidence")
```

We can also specify particular levels of `Variety`:

```{r}
# Specific levels of Variety
ggpredict(m2_TMB, 
          terms = c("Fert", "Variety [Epicure, Ajax]"), 
          type = "random",
          interval = "confidence")
```

Now let's see what happens when we forget to specify `interval = "confidence"`:

```{r}
# Specific levels of Variety
ggpredict(m2_TMB, 
          terms = c("Fert", "Variety [Epicure, Ajax]"), 
          type = "random")
```

The intervals are much wider! That's because these are 95% *prediction* intervals, which are intervals that are expected to cover all individual observations of `Yield`, as opposed to *confidence* intervals, which are measures of uncertainty about the locations of the actual population means. (If this distinction is not familiar to you, you need to revise your 100-level statistics material.)

We can also specify particular levels of both factors - `Fert` and `Variety`:

```{r}
# Specific levels of Fert and Variety
ggpredict(m2_TMB, 
          terms = c("Fert [B]", "Variety [Epicure, Ajax]"), 
          type = "random", 
          interval = "confidence")
```

Now, look what happens when we forget to specify `type = "random"`, thereby omitting the effects of `Variety`:

```{r}
# The same, but forgetting to include `type = "random"`
ggpredict(m2_TMB, 
          terms = c("Fert [B]", "Variety [Epicure, Ajax, Nithsdale]"))

```
They're all the same across the varieties! This is because the random effects are not included unless you include the argument `type = "random"`. 

Right, let's make some plots. 

#### Plot of Variety means

We'll start by plotting the averages of Varieties for the middle Fertiliser type (`Cl`).

```{r}
# estimate means
var_means <- 
  ggpredict(
    m2_TMB, 
    terms = c("Fert [Cl]", "Variety"),
    type = "random",
    interval = "confidence"
    ) |> 
  as.data.frame()

# plot
ggplot() + 
  
  # means and intervals
  geom_pointrange(
    data = var_means,
    mapping = aes(
      x = group,
      y = predicted,
      ymin = conf.low,
      ymax = conf.high,
      colour = group)
  ) +
  
  # data
  geom_point(
    data = potato,
    mapping = aes(
      x = Variety,
      y = Yield,
      colour = Variety
    ),
    pch = 4
  ) +
  
  # prettying
  theme(legend.position = "none") +
  xlab("") +
  ylab("Yield") +
  ggtitle("Predicted mean yields for varieties (with Fert = 'Cl')") +
  coord_flip()

```


#### Plot of Fertiliser means

Now we'll plot the fertiliser means.

```{r}
m2_effects <- ggeffect(m2_TMB) |> as.data.frame()

p2 <- ggplot() +
  geom_pointrange(
    data = m2_effects,
    mapping = aes(x = Fert.x,
                  y = Fert.predicted,
                  ymin = Fert.conf.low,
                  ymax = Fert.conf.high),
    size = 1) +
  geom_point(
    data = potato,
    mapping = aes(x = Fert,
                  y = Yield,
                  group = Variety,
                  colour = Variety),
    pch = 4,
    position = position_dodge(.8)
  ) +
  xlab("Fertiliser") + 
  ylab("Yield")
  
p2

```


#### Plot of Fertiliser-by-Variety means

Now we'll plot the Fertiliser-by-variety means.

```{r}
m2_effects_all <- ggpredict(
  m2_TMB, 
  terms = c("Fert", "Variety"), 
  type = "random",
  interval = "confidence"
  ) |> 
  as.data.frame()

p2 + 
  geom_pointrange(
    data = m2_effects_all,
    mapping = aes(x = x,
                  y = predicted,
                  colour = group,
                  ymin = conf.low,
                  ymax = conf.high),
    position = position_dodge(.8),
    alpha = .7
    ) 
  

```


I think my daughter would like this plot. If there were any unicorn statisticians out there, they'd make plots like this.


#### A note on `marginaleffects`

`marginaleffects` is a useful package for estimating different means of interest. See https://vincentarelbundock.github.io/marginaleffects/index.html for more details. 

Be aware though that, in the past few weeks, there have been some problems highlighted regarding the way standard errors are calculated with `glmmTMB` (<https://github.com/glmmTMB/glmmTMB/issues/915>).



# Exercises

## Tree

The `Tree.txt` dataset available on Stream refers to a two-way factorial design experiment where a researcher investigated the influence of device and operator on the trees height measurements.  Five different types of devices were combined with four operators resulting in 20 combinations (treatments). Each treatment was repeated 10 times, as each combination of device and operator was used to measure 10 trees. 


a)	Read the dataset into R and name the data.frame as “Trees”. You will need to change the data format from “wide” to “long”. Use the following commands to do so.

```{r, eval=TRUE}
Trees <- read.table("https://www.massey.ac.nz/~anhsmith/data/Trees.txt", header = TRUE)
```

```{r}
dtl <- Trees %>% 
  pivot_longer(I:X, 
               names_to = "Tree", 
               values_to="Height")

dtl
```


For the following analysis ignore the column `Tree`.


b) One factor will be treated as random and the other as fixed.	Which one should be treated as random and which should be fixed? Give reasons.

::: {#answer}

Operator is treated as random because we are not very interested in drawing conclusions about these specific operators. Of interest is the degree of variability associated with this factor. On the other hand device is fixed because we want to compare these specific five devices.

:::

c)	Fit a two-way factorial design linear model (using the `lmer` or `glmmTMB` function), including the `Device`, `Operator` and `Device:Operator` terms, and estimate the variance components. Is there any evidence that the random effects explain some variability in tree heights?

::: {#answer}


```{r, echo=TRUE}
library(glmmTMB)

tree_c <- glmmTMB(Height ~ Device + (1|Operator) + (1|Operator:Device), data = dtl)

summary(tree_c)
```

The estimates of the variance components for `Operator:Device` and `Operator` are near zero. It seems that these random effects don't explain much of the variability in tree heights. 

:::


d) In the above analysis, we haven't included the `Tree` effect. Assuming there is no interaction between `Tree` and the other factors, adjust the above model to include `Tree` as a random effect. Estimate the variance components. How do they compare to those you found in c)?

::: {#answer}

```{r}
tree_d <- update(tree_c, .~. + (1|Tree))
summary(tree_d) 

```

The `Operator` variance component got smaller, and the `Operator:Device` variance component got larger. The reflects that fact that the inclusion of an additional term can have unpredictable effects on the other terms in the model, depending on whether the particular variation in $y$  explained by the new factor overlaps or is adjacent to the variation explained by the existing terms. Here, it seems that `Tree` is complementary to the `Operator:Device` term, but takes away from the `Operator` term.

Importantly, the *unexplained* variance went from 8.26 to 0.413, a huge drop! 

For the fixed effects, including `Tree` in the model didn't change the estimates of the coefficients, but it does lower the p-values, due to the `Tree` term explaining away a lot of the previously unexplained variation. 

:::

e)	Using the model in d) as a reference model, test the significance of the random interaction term.

::: {#answer}

To test the hypothesis, $H_0: \sigma_{Operator:Device}^2= 0$, we create a nested model without the `Operator:Device` interaction.

```{r,echo=TRUE}
tree_e1 <- update(tree_d, ~.-(1| Operator:Device))
```

We then do a likelihood ratio test. 

```{r}
anova(tree_e1,tree_d)
```

The p-value of around 0.2 suggests little evidence for the effects of device being different among operators. It may not be high enough to justify removing the term though. 

We could also test the random effect of `Device`. In this case model `tree_e1` will be the reference model.  

```{r,echo=TRUE}
tree_e2 <- update(tree_e1, ~.-(1| Operator))
anova(tree_e2,tree_e1,tree_d)
```

As expected, we can conclude that there is little evidence of variability due to operators.

Since the interaction term was stronger than the main effect of operator, let's test those two directly.

```{r,echo=TRUE}
anova(tree_e2,tree_d)
```
On balance, this would suggest that `Operator` and it's interaction with `Device` are both unnecessary. If this were a real analysis though, I would consult with someone knowledgeable about the system to ask if they'd be comfortable assuming those terms were negligible before deciding to remove them. Absence of evidence isn't evidence of absence!

:::

f)	Assuming `Tree` as the only random effect to be included in your model, test the significance of the fixed factor.

::: {#answer}

```{r}
tree_f <- update(tree_e2, ~.-Device)
anova(tree_f,tree_e2)
```
There is plenty of evidence of differences among devices. 

:::


h) If you have time, use the `ggeffects` or `marginaleffects` package to estimate and plot various means of interest, with 95% confidence intervals. You may have to refit the model with `lme4::lmer` to be able to use the `marginaleffects` package.

::: {#answer}

```{r}

library(marginaleffects)
m <- dtl |> 
  mutate(Tree = fct_reorder(Tree, Height, mean)) |> # reorder trees
  lmer(formula = Height ~ Device + (1 | Tree)) 

preds_by_tree_and_device <- predictions(m)
preds_by_device <- predictions(m, by="Device")

preds_by_tree_and_device |> 
  ggplot() +
  aes(x=Device,col=Tree) +
  geom_point(aes(y=estimate),position=position_dodge(.5), alpha=.7) +
  geom_errorbar(aes(ymin=conf.low,ymax=conf.high),
                position=position_dodge(.5), width=0, alpha=.7) +
  geom_point(aes(y=Height),pch=4,position=position_dodge(.5), alpha=.5) +
  geom_point(aes(y=estimate,col=NULL), 
             preds_by_device, size=2) +
  geom_errorbar(aes(ymin=conf.low, ymax=conf.high, col=NULL),
                preds_by_device,
                position=position_dodge(.5), width=0) +
  ylab("Height")

```

:::







