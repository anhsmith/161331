---
title: "Tutorial 11 - Analysis of repeated measures"
output: 
  html_document:
    number_sections: yes
    toc: yes
    toc_float: yes
    df_print: paged
    code_download: true
---


Note: you can download the .Rmd file using the 'code' button on the top right.

# Setup {-}

```{r setup_rmd, include=F}
knitr::opts_chunk$set(echo = TRUE)
# knitr::opts_knit$set(root.dir = "../data")
knitr::opts_chunk$set(fig.dim=c(5,3.5), out.width="70%", fig.retina=2)
```

```{r message=FALSE}
library(tidyverse)
library(rstatix)
library(ggpubr)
library(glmmTMB)
library(marginaleffects)
library(patchwork)
theme_set(theme_bw())
```

# Analysis of pain data {-}

In this tutorial we'll analyse the data from an experiment on how long the analgesic effects of a drug last on five participants. Pain was measured three times: initially at 3 PM, then at 5 PM, and lastly at 7 PM.

`Pain` is the response vairiable, ranging from 0 = ‘no pain’ to 100 = ‘worst possible pain’.

`Time` (3 PM, 5 PM, 7 PM) is a fixed effect (within subject variable). 

`Subject` is a random effect, crossed with `Time` (which is a fixed effect).

We will analyse this data first with repeated-measures Analysis of Variance, using `rstatix` package, and then we'll move on to modern mixed models, using the `glmmTMB` package.

# Enter the dataset

First, enter the data into R as a `tibble` in long format.

```{r, echo=TRUE}
dt <- tibble(
    Subject = as_factor(rep(1:5,each=3)),
    Time = as_factor(rep(c("3 PM","5 PM","7 PM"),5)),
    Pain = c(45, 50, 55, 42, 42, 45, 36, 41, 43, 39, 35, 40, 51, 55, 59)) |> 
  # reorder subject by their pain scores
  mutate(Subject = fct_reorder(Subject,-Pain,mean)) |> 
  # make another version of time: hours = hours since start of experiment
  mutate(Hour = (as.numeric(Time) - 1) * 2)
dt
```

# Make some initial plots of the data

```{r,echo=TRUE}
gline <- ggplot(dt) +
  aes(x = Time, y = Pain, colour = Subject, group=Subject) +
  # geom_boxplot(aes(col=NULL, group=NULL)) +
  geom_point(size=2,pch=16) +
  geom_line()

gline
```

Another style...

```{r}
gbox <- ggboxplot(dt, x = "Time", y = "Pain", alpha=.5) +
  geom_point(aes(colour = Subject, group=Subject), position = position_dodge(.2)) 

gbox
```


Some observations:

- There are some big differences in pain scores among subjects
- Pain starts low and increases over time

Here are the means and standard deviations of pain scores for each `Time` group.

```{r}
dt |> 
  group_by(Time) |> 
  get_summary_stats(Pain, type = "mean_sd")
```


# Repeated measures analysis of variance

First, we will analyse this dataset with the more traditional, analysis of variance approach.

## Checking the assumption of normality

One of the assumptions of repeated measures ANOVA is normality of residuals. In order to check this assumption we can use the `shapiro_test()` function from `rstatix` package.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
dt |>  
  group_by(Time) |> 
  shapiro_test(Pain) 
```

We can also use a plot to check normality visually.

```{r}
# from the 'ggpubr' package
ggqqplot(dt, "Pain", facet.by = "Time")
```


There doesn't seem to be any evidence of violation of normality assumption, with the p-value from the test being > 0.05 and, in the plot, none of the points are outside where they should be in theory under the assumption of normality.

## Fitting the model and checking the assumption of sphericity

The function `anova_test()` from the `rstatix` package is a convenient function for repeated measures ANOVA. It provides a Mauchly's Test for Sphericity, which is an important assumption of repeated measures ANOVA. If the assumption of sphericity is not met, then we cannot trust the result of the F test for the treatment factor.

Here it is.

```{r, echo=TRUE}

# dv = dependent variable (response,i.e, "Pain")
# wid = variable identifying the subjects: "Subject"
# within = within-subjects variable (fixed factor), i.e, "Time"

anova_pain <- anova_test(data = dt, dv = Pain, wid = Subject, within = Time)
anova_pain
```

There are three parts to this output.

1. The first part (`$ANOVA`) is an ANOVA F-test for the treatment factor, `Time`, under the assumption of sphericity. The null hypothesis is that there are no differences in the mean pain scores among times. The p-value is 0.011, so we can reject the null hypothesis and conclude that the mean pain score is different for at least one pair of times.

2. The second part gives the result of `Mauchly's Test for Sphericity`. Sphericity is a condition where the standard deviations of the between-treatment differences (3 PM vs 5 PM, 3 PM vs 7 PM, and 5 PM vs 7 PM) are all the same.

Below, I've calculated the differences in pain scores between the times.

```{r}
diff_dt <- dt |> 
  group_by(Subject) |> 
  summarise(`5PM, 3PM` = Pain[Time=="5 PM"] - Pain[Time=="3 PM"],
            `7PM, 3PM` = Pain[Time=="7 PM"] - Pain[Time=="3 PM"],
            `7PM, 5PM` = Pain[Time=="7 PM"] - Pain[Time=="5 PM"]) |> 
  pivot_longer(cols=2:4, names_to = "Time pairs", values_to = "Differences in pain scores")

diff_dt
```

Now let's calculate the raw standard deviations. Ideally, under sphericity, these would be similar (or rather, *identical* in the population).

```{r}
diff_dt |> group_by(`Time pairs`) |> summarise(sd(`Differences in pain scores`))
```

We can also plot the differences to see how different the spreads of points are.

```{r}
diff_dt |> ggplot() + 
  aes(x=`Time pairs`,y=`Differences in pain scores`) + 
  geom_boxplot() +
  geom_jitter(col=2,width=.2)
```

Does it look like sphericity is true here? The standard deviations of the differences are actually quite different among the pairs of times. But let's look at the test again.

```{r , results='asis'}
anova_pain$`Mauchly's Test for Sphericity`
```

For this test, the null hypothesis is that sphericity is true. The result of the formal test (in the second part of the output of the `anova_test`) is that we *do not* reject the null hypothesis (p = 0.158). There is no evidence against the assumption of sphericity, so we can report the above p-value of 0.011 for the ANOVA test of the `Time` factor. However, this failure to reject the null might be due to low power, given the small sample size.

If the p-value of Mauchly's test was < 0.05, we could not assume sphericity. In that case, we would look at the third part for out test of the `Time` effect.

3. Let's look at the third part again.

```{r , results='asis'}
anova_pain$`Sphericity Corrections`
```

This part shows a test of the effect of `Time` that has been *adjusted for any departure from the assumption of sphericity*. The degree of non-sphericity determines the size of the correction. There are two methods, Greenhouse-Geisser (GG) and Huynh-Feldt (HF), with the former being more severe (in this case, at least). You can see that the p-values for the effect of Time after correcting for departure from sphericity are larger than the p-value without correction. The corrected test is more conservative. But, in this case, the effect of Time on pain scores is still significant at the 5% level. 

## Post-hoc analysis

The above analysis tests for an overall effect of the `Time` factor. But which times are different from which others?

We can do pair-wise comparisons of each pair of times, adjusting for multiple comparisons, as follows. 

```{r}
pwc <- dt %>%
  pairwise_t_test(
    Pain ~ Time, paired = TRUE,
    p.adjust.method = "bonferroni"
    )
pwc
```
This says that we have little evidence of a difference in mean pain scores between times 3PM and 5PM (p.adj = 0.957), weak evidence between 3PM and 5PM (p.adj = 0.074), and strong evidence between 5PM and 7PM (p.adj = 0.009). 

We can show this graphically in the traditional style like so.

```{r }
pwc <- pwc %>% add_xy_position(x = "Time")
gbox + 
  stat_pvalue_manual(pwc) +
  labs(
    subtitle = get_test_label(anova_pain, detailed = TRUE),
    caption = get_pwc_label(pwc)
  ) +  theme(legend.position="none")
```


The two asterisks "**" above the horizontal line joining the 5 PM and 7 PM groups indicate that this pair is significantly different. The other pairs have "ns", indicating they are not.

The overall conclusion? It looks like there is a difference in subjects' mean pain scores over time, particularly between times 5 PM and 7 PM. 


# Linear mixed model

Another approach is to used modern mixed models.

## `lmm1`: A basic mixed model

### Describing and fitting the model

$$
y_{ij} = \mu + \alpha_{i} + b_j  + \varepsilon_{ij} \\
b_j \sim N(0,\sigma^2_B) \\
\varepsilon_{ij} \sim N(0,\sigma^2_\varepsilon)
$$

where

$y_{ijk}$ is the pains score from Time $i$, Subject $j$,

$\alpha_{i}$ is the effect of Time $i$,

$b_{j(i)}$ is the effect of Subject $j$,

$\sigma^2_B$ is the variance component for Subject,

$\varepsilon_{ij}$ are the error terms, and

$\sigma^2_\varepsilon$ is the error variance.


This is a basic mixed model with a random effect of subject and fixed effect of time. The assumption is that subjects get their own random intercepts, applied across all three time periods. Any deviation from the additive effects of time and subject go into the error term. 

Now fit the model.

```{r}
lmm1 <- glmmTMB(Pain ~ Time + (1 | Subject), data=dt)
summary(lmm1)
```

### Interpretation of the fixed effect of `Time`

We estimate the average pain score at 3 PM to be 42.6, increasing by 2 to 44.6 at 5 PM, and finally increasing by 5.8 (relative to 3 PM) to 48.4 at 7 PM. 

We can extract these predictions with confidence intervals like so.

```{r}
# refit the model with lme4 to avoid issues with 
# glmmTMB not playing with marginaleffefcts
lmer1 <- lme4::lmer(Pain ~ Time + (1 | Subject), data=dt)

predf <- predictions(lmer1, by="Time")
```

And plot the marginal means with confidence intervals (after 'removing' subject effects) in black as follows. 

```{r}
gline +
  geom_point(
    data = predf, 
    aes(x=Time, y=estimate), inherit.aes = F, size=4
    ) +
  geom_line(
    data =predf, 
    aes(x=as.numeric(Time), y=estimate), 
    inherit.aes = F
    ) +
  geom_errorbar(
    data =predf, 
    aes(x=Time, ymin=conf.low, ymax=conf.high), 
    width=0, 
    inherit.aes = F
    ) 
```

It looks like the average pain increased slightly more from 5 and 7 PM then from 5 and 3 PM. Although, with such a small dataset, this may be driven solely by Subject 4, who seemed to be feeling unusually good at 5 PM.

### Interpreting the random effects

The usual method to extract random effects ($b_j$) is with the `ranef` function.

```{r}
ranef(lmm1)
```

These represent the differences between the estimated pain scores of the subjects at 3 PM and the overall mean pain score at 3 PM. 

The `marginaleffects` package provides a lot more ways to extract estimates from our models, with confidence intervals.

```{r}
predr <- predictions(lmer1, by="Subject") |> 
  # subtract the mean to get marginal effects
  mutate(pred.mar = estimate - mean(estimate),
         conf.low.mar = conf.low - mean(estimate),
         conf.high.mar = conf.high - mean(estimate))
predr
```

Let's plot the subject effects and confidence intervals.

```{r , fig.width=5, fig.height=1.8}
predr |> 
  ggplot() + 
  aes(x=Subject,y=pred.mar,col=Subject) +
  geom_point() +
  geom_errorbar(aes(ymin=conf.low.mar,
                    ymax=conf.high.mar),
                width=0) +
  ylab("Subject effects") +
  theme(legend.position="none") + coord_flip()
```


And now let's plot the whole lot together...

```{r}
predall <- predictions(lmer1)
predall

```

```{r}
ggplot(predall) +
  aes(x = Time, 
      y = estimate, 
      col=Subject, 
      group=Subject
      ) +
  geom_ribbon(
    aes(ymin=conf.low, 
        ymax=conf.high, 
        col=NULL,
        fill = Subject), 
    width=0, alpha=.3
    ) +
  geom_line() + 
  geom_point(data=dt, 
             aes(y=Pain)
             ) +
  geom_point(data =predf, 
             aes(x=Time, y=estimate), 
             inherit.aes = F, size=4
             ) +
  geom_line(data =predf, 
            aes(x=as.numeric(Time), y=estimate), 
            inherit.aes = F
            ) +
  geom_errorbar(data =predf,
                aes(x=Time, ymin=conf.low, ymax=conf.high),
                width=0, inherit.aes = F)
 
```

In the above plot, we have predictions from the model for each subject in each time, with confidence intervals. The coloured points are the observed values. The black represents the predicted overall marginal mean. 

## `mixreg1`: A mixed-effects regression model with (correlated) random intercepts and random slopes

The trend in pain through time looks quite close to linear. Do we need to treat time as a factor? Or would it be better to treat time as a numerical variable? The advantage of treating `Time` as a numerical variable rather than a factor is that it would use only 1 degree degree of freedom rather than two. This counts for a lot in an unreplicated design. It might allow us to fit an interaction between `Time` and `Subject`, which we cannot do otherwise. 

Here's a description of the model I propose. 


$$
y_{ij} = \alpha + \beta x_{ij} + a_i + b_i x_{ij}  + \varepsilon_{ij} \\
a_i \sim N(0,\sigma^2_a) \\
b_i \sim N(0,\sigma^2_b) \\
\varepsilon_{ij} \sim N(0,\sigma^2_\varepsilon)
$$

where

$~~~~y_{ij}$ is the pain score for Subject $i$ in observation $j$,

$~~~~x_{ij}$ is the number of hours since the beginning of the experiment for observation $ij$,

$~~~~\alpha$ is the population-level intercept (mean pain at $x = 0$),

$~~~~\beta$ is the population-level slope for pain - the average change in pain per hour,

$~~~~a_{i}$ is the random effect of Subject $i$ on the intercept,

$~~~~b_{i}$ is the random effect of Subject $i$ on the slope,

$~~~~\sigma^2_a$ is the variance component for the random effect of Subject on the intercept,

$~~~~\sigma^2_b$ is the variance component for the random effect of Subject on the slope, and

$~~~~\varepsilon_{ij}$ are the errors and $\sigma^2_\varepsilon$ the error variance.



We'll use the numerical variable `Hour` as our predictor here.  

```{r}
# fit the model
mixreg1 <- glmmTMB(Pain ~ Hour + (Hour | Subject), data = dt)
summary(mixreg1)
```

Oops, there was a warning that the model had trouble converging. It is slightly overparameterised. Let's think about why. 

This model has, at its core, a population-level regression $\hat{y} = \alpha+\beta x$, averaged across subjects. Here, the estimates are $\hat{y} = 42.3 + 1.45 x$.

On top of the population-level model, there are subject-level random effects on the intercept (with $\hat\sigma_a$ of 5.08) *and* subject-level random effects on the slope (with $\hat\sigma_b$ of 0.60). This is essentially an interaction, where each subject has their own regression model, given by $\hat{y} = (\alpha+a_i)+(\beta+b_i) x$. To get the subject-specific intercept and slope parameters, we simply add the subject-specific random effects to the appropriate population-level parameter. 

Here are those random effects:

```{r}
ranef(mixreg1)
```


There's one parameter this model has fit that is not in the model I articulated above: `Corr`. This is a correlation parameter between the $a_i$ and the $b_i$. The estimate for this correlation is = 1, which is very strange. It means that the random slopes and random intercepts are perfectly correlated. 

Here's the correlation for the random effects of intercepts and slopes:

```{r}
ranef(mixreg1)[[1]]$Subject |> cor()
```
Here are the lines that are being fit to each subject.

```{r, warning=F}
lmer_mixreg1 <- lme4::lmer(Pain ~ Hour + (Hour | Subject),
                            data = dt)

plot_cap(lmer_mixreg1, 
         condition = c("Hour", "Subject")
         ) +
  geom_point(data=dt, 
             aes(x=Hour,y=Pain,col=Subject))
```

I would not trust this model! It didn't converged properly, either with `glmmTMB` or `lmer`. It looks like a terrible fit to subject 4. And the complete correlation between intercepts and slopes makes it look pretty weird.

The term `(Hour | Subject)` in the formula automatically fits a random intercept and a random effect on the slope for each subject. It is equivalent to `(1 + Hour | Subject)`. When two effects (here, intercept and slope) are specified in the same term like this, the model automatically fits a correlation.

Instead of fitting the $a_i$ and $b_i$ separately, as we specified above with $a_i \sim N(0,\sigma^2_a)$ and $b_i \sim N(0,\sigma^2_b)$, it has fit $a_i$ and $b_i$ as a multivariate normal distribution ($\text{MVN}$):

$$
\begin{equation}
\begin{bmatrix}
  a_i \\
  b_i 
  \end{bmatrix}
\sim \text{MVN}(\begin{bmatrix}
  0 \\
  0\\
  \end{bmatrix},
\begin{bmatrix}
  \sigma^2_a & \\
  \sigma_{ab} & \sigma^2_b \\
  \end{bmatrix}
)
\end{equation}
$$
where $\sigma_{ab}$ is the covariance between the random effects on the intercept and slope across subjects. (It actually parameterised it as a correlation, rather than a covariance, but this is just a direct transformation.)

This extra correlation/covariance parameter, which we didn't specifically ask for, seems a bridge too far for this model. 

## `mixreg2`: A mixed-effects regression model with uncorrelated random intercepts and random slopes

We can remove the correlation from the model by decoupling the intercept term from the slope term.

The term `(0 + Hour | Subject)` fits a random effect on the slope but not an intercept. 

The term `(1 | Subject)` fits a random intercept but no slope effect. 

So, let's put these effects into the model separately.  

```{r}
mixreg2 <- glmmTMB(
  Pain ~ Hour + (1 | Subject) + (0 + Hour | Subject), 
  data = dt)

summary(mixreg2)
```

The correlation parameter has now been removed and the model, and it converged fine. The `df.resid` has increased from 9 to 10. And we have a test for the population-level slope of `Hour`, and it is highly significant with a p-value of 0.0003. 

```{r}
lmer_mixreg2 <- lme4::lmer(
  Pain ~ Hour + (1 | Subject) + (0 + Hour | Subject), 
  data = dt)

lmer_predf_mixreg2 <- predictions(lmer_mixreg2, by="Hour")
lmer_predf_mixreg2 |> summary()

```

Now let's plot the lines fit per subject, adding the raw data and population-level predictions. 

```{r}
g1 <- plot_cap(
    lmer_mixreg2, 
    condition = c("Hour", "Subject")
    ) +
  # add raw data
  geom_point(
    data = dt, 
    aes(x=Hour,y=Pain,col=Subject)
    ) +
  # add overall mean
  geom_ribbon(
    data = lmer_predf_mixreg2,
    aes(x=Hour,ymin=conf.low,ymax=conf.high), 
    alpha=.2
    ) +
  geom_line(
    data = lmer_predf_mixreg2, 
    aes(x=Hour, y=estimate), 
    size=1)

g1
```

We can see that the model has fit slightly different slopes (and intercepts) to each subject. We can get the random effects on the intercept (`(Intercept)` = $a_i$) and slope (`Hour` = $b_i$) with the `ranef()` function. 

```{r}
ranef(mixreg2)

ranef(lmer_mixreg2)
```

Or use the `marginaleffects()` function to extract the subject-specific slopes ($\beta + b_i$).

```{r}
marginaleffects(lmer_mixreg2, by="Subject") |>  summary()
```

The fitted slopes for each subject are in the column headed 'Effect'. The estimated slope for subject 1 was 2.09 (with a 95% confidence interval of 1.66, 2.52), subject 2 was 1.04 (0.65, 1.43), etc. This can be interpreted as the pain increasing by 2.09 units per hour for subject 1, and 1.04 units per hour for subject 2, etc. 

### Residuals

We can look at the residuals by `Time` and/or `Subject`. 

```{r  out.width="100%"}

resid_by_hour <- 
  predictions(lmer_mixreg2) |> 
  mutate(Residuals = Pain - estimate) |> 
  ggplot() +
  geom_hline(yintercept=0) +
  aes(x=Hour,y=Residuals,col=Subject) +
  geom_point() 

resid_by_subject <- 
  predictions(lmer_mixreg2) |> 
  mutate(Residuals = Pain - estimate,
         Hour = as_factor(Hour)) |> 
  ggplot() +
  geom_hline(yintercept=0) +
  aes(x=Subject,y=Residuals,col=Hour) +
  geom_point()

resid_by_hour + resid_by_subject
```
Variance of the residuals seemed to decline over time, perhaps. And there may be a pattern of high residuals at hour = 0, low at hour = 2, then high at low = 4. There is not a lot in this though, and it is driven by just a few points. If you removed Subject 4, both these plots would look fine. 


### `lmm3`: A linear mixed model with random intercepts and fixed slope

We might ask: do we need the model to have different slopes for each subject?

Let's fit a model without random effects on the slope. We'll retain the random effects of subjects on the intercept (i.e., the height of the line) but we'll assume the slope applies to all subjects ($b_i=0,~~\forall~~i$). They all have the average change in pain per hour.

```{r}
lmm3 <- glmmTMB(Pain ~ Hour + (1 | Subject), data = dt)
summary(lmm3)
```

Let's see whether there is any evidence in support of having the random slopes term. 

```{r}
anova(lmm3,mixreg2)
```
The p-value is fairly low here. Does that mean we should assume that subjects have *exactly the same* average rate of change in pain per hour? 

Which seems like the stronger assumption: change in pain over time (1) is exactly the same for everyone or (2) differs among people. 

To me, it seems like a stronger assumption to assert that everyone's pain changes at the same rate. So, even though we don't have any firm evidence in support of the more complex model with varying slopes, I would be more comfortable allowing slopes to vary among subjects (and estimating that variance) than asserting there is only one slope for all. 


### Investigation of shrinkage effects

Recall that, when an effect is treated as random rather than fixed, there is some 'shrinkage' of the effects. That is, the estimates of the effects will be smaller when treated as random than they are when treated as fixed. 

Let's look into this in the context of our model and the random effects of subjects on the slope and intercept. 

First, we'll need to fit a linear fixed-effects model, and then we'll need to extract the parameter estimates for Subject on the slope and intercept of the relationship between pain and hours. 

```{r}
# fit a fixed-effects linear model with interaction
flm <- lm(Pain ~ Hour * Subject, data = dt)
```


```{r}
marginaleffects(
    flm,
    variable="Hour",
    by="Subject"
    ) #|> 
  # transmute(
  #     Subject, 
  #     Value="contrast", 
  #     Parameter="Slope", 
  #     Model="Fixed"
  #     )

marginaleffects(
    lmer_mixreg2,
    variable="Hour",
    by="Subject"
    )
```


```{r}
# Extract intercepts and slopes from fixed and mixed models
pars_fix_rand <- bind_rows(
  
  marginaleffects(
    flm,
    variable="Hour",
    by="Subject"
    ) |> 
    transmute(
      Subject, 
      Value="contrast", 
      Parameter="Slope", 
      Model="Fixed"
      ),
  
  marginaleffects(
    lmer_mixreg2,
    variable="Hour",
    by="Subject"
    ) |> 
    transmute(
      Subject, 
      Value=dydx, 
      Parameter="Slope", 
      Model="Random"
      ),
  
  predictions(flm) |> 
    filter(Hour == 0) |> 
    transmute(
      Subject,
      Value=estimate,
      Parameter="Intercept",
      Model="Fixed"
      ),
  
  predictions(lmer_mixreg2) |> 
    filter(Hour == 0) |> 
    transmute(
      Subject,
      Value=estimate,
      Parameter="Intercept",
      Model="Random")
  )

# Plot them
ggplot(pars_fix_rand) +
  aes(
    x=Model,
    y=Value,
    col=Subject,
    group=Subject
    ) +
  geom_point() + 
  geom_line() +
  facet_wrap(.~Parameter, scales="free") +
  ggtitle("Shrinkage of Subject effects on intercept and slope parameters",
    subtitle = "Comparison of estimates between fixed-effects and mixed-effects models")
```


We can see that there is no real shrinkage of the intercept term (left). However, we can see some quite substantial shrinkage of the effects of subject on slopes (right) - there is much less variation in the slopes among subjects when this term is treated as random rather than fixed.

Let's see the degree to which the slope and intercept parameters are correlated.

```{r}
pars_fix_rand |> 
  pivot_wider(values_from = Value, names_from = Parameter) |> 
  ggplot() +
  aes(x=Intercept,y=Slope,col=Subject) +
  geom_point() +
  facet_wrap(~Model) +
  ggtitle("Intercept vs slope estimates for fixed model vs mixed model")
```


In the fixed-effects model, the intercept and slope don't appear very correlated. In the mixed-effects model, there does seem to be a correlation - the subjects with higher intercepts generally had higher slopes. Subjects that stated with more pain tended to have faster increase in pain over time. With only five subjects, this is far from conclusive.

Finally, let's compare the actual regression lines between the fixed vs mixed models.

```{r}
# get predictions from fixed and random models
preds_fix_rand <- bind_rows(
  predictions(mixreg2) |> 
    select(Subject, Hour, Pain=predicted) |> 
    mutate(Model="Random") ,
  predictions(flm) |> 
    select(Subject, Hour, Pain=predicted) |> 
    mutate(Model="Fixed")
)

# plot data and predictions
dt |> 
  ggplot() +
  aes(x=Hour,y=Pain,col=Subject) +
  geom_point() +
  geom_line(aes(linetype=Model),
            data = preds_fix_rand) +
  ggtitle("Regression lines by subject for fixed model vs mixed model")
```

### Comparison of models with Time as a factor vs Time as a number

How does `lmm1` differ from `mixreg2`? In `lmm1`, we treated Time as a factor; whereas, in `mixreg2`, we treated time as a numerical predictor.

When we fit a linear model to a numerical predictor, the effect on $y$ of increasing $x$ by 1 unit is constant. In the context of model `mixreg2`, that means that the change in pain was restricted to be the same from 3 PM and 5 PM as from 5 PM to 7 PM. 
In contrast, when we fit a model to a factor, there is no such restriction. The levels of the factor are 'exchangeable', meaning there is no inherent order to the levels. The means of $y$ fitted to the levels can increase or decrease more or less independently.

So which model is more complex, and which provided a better fit? 

We can use the `anova` function. 

```{r}
anova(lmm1,mixreg2)
```


Both these models have the same number of parameters, so it comes down to which has the better fit to the data. There is very little in it, but the mixed regression model that allowed for constant, linear change in pain through time, with the slope and intercept randomly varying among subjects, provided a slightly better fit to the data (lower deviance). 

# Exercise: revisiting the mixed model with `Time` as a factor

Now, let's fit to this dataset some of the other mixed models that we fit to the arthritis dataset in the lecture for week 11. 

1. Fit two mixed models with varying effects of subject, where the effects of subject are different among times ($b_{ij}$, instead of $b_j$). Fit this model with 
  - a compound symmetric variance-covariance matrix (model `lmm2` in the lecture), and 
  - an unstructured variance-covariance matrix (model `lmm3` in the lecture). 


Remember, you're now fitting 5 x 3 = 15 subject effects, so be sure to remove the error term from the model! 
    
2. Compare the mixed models using `anova`. Which of these models is 'best'? 

3. Plot some of the predictions from the best `lmmx` model.

4. Finally, compare the best `lmmx` model to the `mixreg2` model. 

